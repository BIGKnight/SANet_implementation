{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import utils\n",
    "import SANet_model\n",
    "result_output = open(\"/home/zzn/SANet_implementation-master/result_B_12.13.txt\", \"w\")\n",
    "image_train_path = \"/media/zzn/922E52FA2E52D737/SANet/part_B_final/train_data/images_train.npy\"\n",
    "gt_train_path = \"/media/zzn/922E52FA2E52D737/SANet/part_B_final/train_data/gt_train.npy\"\n",
    "image_validate_path = \"/media/zzn/922E52FA2E52D737/SANet/part_B_final/train_data/images_validate.npy\"\n",
    "gt_validate_path = \"/media/zzn/922E52FA2E52D737/SANet/part_B_final/train_data/gt_validate.npy\"\n",
    "batch_size = 1\n",
    "epoch = 300\n",
    "loss_c_weight = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    image_train = np.load(image_train_path)\n",
    "    gt_train = np.load(gt_train_path)\n",
    "    image_validate = np.load(image_validate_path)\n",
    "    gt_validate = np.load(gt_validate_path)\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=[None, None, None, 3], name=\"input\")\n",
    "    y = tf.placeholder(tf.float32, shape=[None, None, None, 1], name=\"label\")\n",
    "    estimated_density_map = SANet_model.scale_aggregation_network(x)\n",
    "    estimated_counting = tf.reduce_sum(estimated_density_map, reduction_indices=[1, 2, 3], name=\"crowd_counting\")\n",
    "    ground_truth_counting = tf.reduce_sum(y, reduction_indices=[1, 2, 3])\n",
    "    ground_truth_counting = tf.cast(ground_truth_counting, tf.float32)\n",
    "\n",
    "    loss_e = tf.losses.mean_squared_error(y, predictions=estimated_density_map)\n",
    "    loss_c = utils.structural_similarity_index_metric(estimated_density_map, y)\n",
    "    loss = tf.add(loss_e, tf.multiply(loss_c_weight, loss_c))\n",
    "\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=1e-5).minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "    eval_metric_ops = {\n",
    "        'MAE': tf.reduce_mean(tf.abs(tf.subtract(estimated_counting, ground_truth_counting)), axis=0, name=\"MAE\"),\n",
    "        'MSE': tf.reduce_mean(tf.square(tf.subtract(ground_truth_counting, estimated_counting)), axis=0, name=\"MSE\"),\n",
    "    }\n",
    "\n",
    "    MAE = 19970305\n",
    "    MSE = 19970305\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    with tf.Session() as sess:\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "        image_train_num = len(image_train)\n",
    "        step = 0\n",
    "        for i in range(epoch):\n",
    "            for j in range(image_train_num // batch_size):\n",
    "\n",
    "                # train\n",
    "                start = (j * batch_size) % image_train_num\n",
    "                end = min(start + batch_size, image_train_num)\n",
    "                sess.run(train_op, feed_dict={x: image_train[start:end], y: gt_train[start:end]})\n",
    "                step = step + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                # validate\n",
    "                if step % 50 == 0:\n",
    "                    loss_ = []\n",
    "                    MAE_ = []\n",
    "                    MSE_ = []\n",
    "                    for k in range(len(image_validate // batch_size)):\n",
    "                        loss_eval, metric_eval = sess.run([loss, eval_metric_ops], feed_dict={x: image_validate[k:k+batch_size], y: gt_validate[k:k+batch_size]})\n",
    "                        loss_.append(loss_eval)\n",
    "                        MAE_.append(metric_eval['MAE'])\n",
    "                        MSE_.append(metric_eval['MSE'])\n",
    "                    loss_ = np.mean(loss_)\n",
    "                    MAE_ = np.mean(MAE_)\n",
    "                    RMSE = np.sqrt(np.mean(MSE_))\n",
    "                    print(loss_)\n",
    "                    print(\"MAE: \" + str(MAE_), \"MSE: \" + str(RMSE), \"CURRENT_BEST_VALIDATING_MAE: \" + str(MAE))\n",
    "                    result_output.write(str(loss_) + \"      \" + \"MAE: \" + str(MAE_) + \" MSE: \" + str(RMSE) + \"\\r\\n\")\n",
    "\n",
    "                    # save model\n",
    "                    if MAE > MAE_:\n",
    "                        MAE = MAE_\n",
    "                        saver.save(sess, './checkpoint_dir/MyModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_output.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
